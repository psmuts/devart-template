I have been thinking a lot about where the content/inventory of words will come from in addition to user contributions and the implications of this decision.  My initial idea was to use words exclusively contributed by participants, but the more I thought about this the more I realized this was less encompassing (and less interesting) that I initially hoped.  Firstly, because using contributions from contributors exclusively, excludes more possibilities than it creates because a) English speakers use only about 12,000 words of the more than 171,000 in the OED and b) because, as Cathy O'Neil and Rachel Schutt argue in their excellent book "Doing Data Science," n != all.  They say it better than I could: 


"Here’s the thing: it (N sample size) is pretty much never all. And we are very often
missing the very things we should care about most.

An example from that very article—election night polls—is in itself a
great counter-example: even if we poll absolutely everyone who leaves
the polling stations, we still don’t count people who decided not to vote
in the first place. And those might be the very people we’d need to talk
to to understand our country’s voting problems.
Indeed, we’d argue that the assumption we make that N=ALL is one
of the biggest problems we face in the age of Big Data. It is, above all,
a way of excluding the voices of people who don’t have the time, energy,
or access to cast their vote in all sorts of informal, possibly unannounced,
elections.

Those people, busy working two jobs and spending time waiting for
buses, become invisible when we tally up the votes without them. To
you this might just mean that the recommendations you receive on
Netflix don’t seem very good because most of the people who bother
to rate things on Netflix are young and might have different tastes than
you, which skews the recommendation engine toward them. But there
are plenty of much more insidious consequences stemming from this
basic idea."


So, in order to get a wide sampling of words in use without exclusively relying on the imaginations of voluntary contributors I decided to use both an external modernized dictionary from Wordnik and to harvest tags from social networking platforms.   

Wordnik offers an API to their datastore of words from both traditional dictionaries as well as words in common usage but without formal definitions.  Using their API, I compiled a dictionary of words in 9 types (noun, adjective, adverb, verb, pronoun, idiom, proper-noun, interjections, conjunctions and prepositions).  These include words from "indoctrinate," "polyethylene," and "Benjamin Franklin" to "geekster, "Dumbledore," and "sneezy."  I have a randomized selection of 94,000 words, classified by type, currently in use.  

Finally, I am harvesting tags from Google+, FB and Twitter and adding unique occurrences to my word database, though I am still working on criteria for selecting hashtags.  

We continue to hear about the ways language is changing (some would say de-evolving) because of the new modes of articulation enabled by the web.  Whether it is the idea of everyone posting the most mundane aspects of life in novelly cryptic ways on FB,G+ or the every evolving language of texting (http://www.ted.com/talks/john_mcwhorter_txtng_is_killing_language_jk)  I wanted to find a way then to throw these various linguistic worlds together with the existing richness of the English language. It is fun, surprising and stimulating to see the blending of new idioms and expressions from informal speech, posts to social networks, hash tagging, text-speech etc. mix with more "formal" language. 

